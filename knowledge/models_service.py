import os
import json
import time
import requests
import tiktoken
import openai
from django.utils import timezone
from django.db import transaction
from typing import List, Dict, Any, Optional, Tuple

from knowledge.ai_models import ModelProvider, AIModel, TokenUsage

class TokenCounter:
    """Tokenè®¡æ•°å·¥å…·"""
    @staticmethod
    def count_tokens(text: str, model_name: str) -> int:
        """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
        try:
            encoding = tiktoken.encoding_for_model(model_name)
            return len(encoding.encode(text))
        except Exception:
            try:
                # å›é€€åˆ°cl100k_baseç¼–ç 
                encoding = tiktoken.get_encoding("cl100k_base")
                return len(encoding.encode(text))
            except Exception:
                # æœ€åŸºç¡€çš„è¿‘ä¼¼ç®—æ³•ï¼šæŒ‰ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å•è¯è®¡ç®—
                chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
                english_words = len([w for w in text.split() if all(c.isalpha() for c in w)])
                return chinese_chars + english_words

    @staticmethod
    def count_message_tokens(messages: List[Dict[str, str]], model_name: str) -> int:
        """è®¡ç®—æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°é‡"""
        total_tokens = 0
        for message in messages:
            # æ¯æ¡æ¶ˆæ¯çš„è§’è‰²å’Œå†…å®¹
            role_tokens = TokenCounter.count_tokens(message.get('role', ''), model_name)
            content_tokens = TokenCounter.count_tokens(message.get('content', ''), model_name)
            # æ¯æ¡æ¶ˆæ¯é¢å¤–è®¡ç®—4ä¸ªtokensä½œä¸ºæ ¼å¼å¼€é”€
            total_tokens += role_tokens + content_tokens + 4
        
        # é¢å¤–æ·»åŠ 2ä¸ªtokensä½œä¸ºæ ¼å¼å¼€é”€
        return total_tokens + 2

class ModelService:
    """å¤§æ¨¡å‹æœåŠ¡åŸºç±»"""
    def __init__(self, model_id: str):
        # ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®
        try:
            self.model_config = AIModel.objects.select_related('provider').get(model_id=model_id)
            self.model_name = self.model_config.name
            self.provider = self.model_config.provider
            self.is_active = self.model_config.is_active and self.provider.is_active
        except AIModel.DoesNotExist:
            raise ValueError(f"æ‰¾ä¸åˆ°æ¨¡å‹é…ç½®: {model_id}")
        
    def generate_response(self, messages: List[Dict[str, str]], user=None, conversation=None, message=None) -> Tuple[str, Dict]:
        """ç”Ÿæˆå“åº”ï¼Œç”±å­ç±»å®ç°"""
        raise NotImplementedError
        
    @staticmethod
    def get_service(model_id: str) -> 'ModelService':
        """å·¥å‚æ–¹æ³•ï¼Œæ ¹æ®æ¨¡å‹IDè¿”å›å¯¹åº”çš„æœåŠ¡"""
        print(f"ğŸ‘‰ å°è¯•è·å–æ¨¡å‹æœåŠ¡ - æ¨¡å‹ID: {model_id}")
        
        try:
            # æŸ¥è¯¢æ¨¡å‹é…ç½®
            print(f"   æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ¨¡å‹é…ç½®...")
            model_config = AIModel.objects.select_related('provider').get(model_id=model_id)
            print(f"   âœ… æ‰¾åˆ°æ¨¡å‹é…ç½®: {model_config.name} (ID: {model_config.model_id})")
            
            # æ ¹æ®æä¾›å•†ç±»å‹é€‰æ‹©æœåŠ¡ç±»
            provider_slug = model_config.provider.slug
            print(f"   ğŸ“‹ æä¾›å•†ä¿¡æ¯: {model_config.provider.name} (slug: {provider_slug})")

            # æ ¹æ®æä¾›å•†ç±»å‹åˆ›å»ºç›¸åº”çš„æœåŠ¡å®ä¾‹
            print(f"   ğŸ”§ å‡†å¤‡åˆ›å»ºæ¨¡å‹æœåŠ¡å®ä¾‹ ({provider_slug})...")
            
            if provider_slug == 'openai':
                print(f"   ğŸš€ åˆ›å»º OpenAI æœåŠ¡å®ä¾‹")
                return OpenAIService(model_id)
            elif provider_slug == 'aliyun':
                print(f"   ğŸš€ åˆ›å»ºé˜¿é‡Œäº‘æœåŠ¡å®ä¾‹")
                return AliyunService(model_id)
            elif provider_slug == 'deepseek':
                print(f"   ğŸš€ åˆ›å»º DeepSeek æœåŠ¡å®ä¾‹")
                return DeepSeekService(model_id)
            else:
                print(f"   âŒ ä¸æ”¯æŒçš„æä¾›å•†ç±»å‹: {provider_slug}")
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æä¾›å•†: {provider_slug}")
                
        except AIModel.DoesNotExist:
            # å¦‚æœæ‰¾ä¸åˆ°æ¨¡å‹ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
            print(f"   âš ï¸ æ¨¡å‹ID: {model_id} åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨")
            print(f"   ğŸ” å°è¯•è·å–é»˜è®¤æ¨¡å‹...")
            
            default_model = AIModel.objects.filter(is_default=True).first()
            if default_model:
                print(f"   ğŸ“ ä½¿ç”¨é»˜è®¤æ¨¡å‹: {default_model.name} (ID: {default_model.model_id})")
                return ModelService.get_service(default_model.model_id)
            else:
                print(f"   âŒ ç³»ç»Ÿä¸­æ²¡æœ‰è®¾ç½®é»˜è®¤æ¨¡å‹")
                
                # å°è¯•è·å–ä»»æ„æ´»è·ƒæ¨¡å‹
                print(f"   ğŸ” å°è¯•è·å–ä»»æ„æ´»è·ƒæ¨¡å‹...")
                any_model = AIModel.objects.filter(is_active=True).first()
                if any_model:
                    print(f"   ğŸ“ ä½¿ç”¨æ´»è·ƒæ¨¡å‹: {any_model.name} (ID: {any_model.model_id})")
                    return ModelService.get_service(any_model.model_id)
                else:
                    print(f"   âŒ ç³»ç»Ÿä¸­æ²¡æœ‰ä»»ä½•å¯ç”¨æ¨¡å‹")
                    raise ValueError(f"æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä¸”æ²¡æœ‰é»˜è®¤æ¨¡å‹")
        except Exception as e:
            print(f"   âŒ è·å–æ¨¡å‹æœåŠ¡æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            raise
    
    @staticmethod
    def get_default_model() -> str:
        """è·å–é»˜è®¤æ¨¡å‹ID"""
        default_model = AIModel.objects.filter(is_default=True).first()
        if default_model:
            return default_model.model_id
        else:
            # å¦‚æœæ²¡æœ‰è®¾ç½®é»˜è®¤æ¨¡å‹ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ´»è·ƒæ¨¡å‹
            active_model = AIModel.objects.filter(is_active=True).first()
            if active_model:
                return active_model.model_id
            else:
                raise ValueError("ç³»ç»Ÿä¸­æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹")

    def record_token_usage(self, user, conversation, message, 
                          prompt_tokens, completion_tokens, 
                          response_time, is_successful=True, 
                          error_message="", metadata=None):
        """è®°å½•Tokenä½¿ç”¨æƒ…å†µ"""
        total_tokens = prompt_tokens + completion_tokens
        
        # è®¡ç®—æˆæœ¬
        cost_prompt = (prompt_tokens / 1000) * self.model_config.cost_prompt
        cost_completion = (completion_tokens / 1000) * self.model_config.cost_completion
        cost_usd = cost_prompt + cost_completion
        cost_rmb = cost_usd * 7.2  # ç¾å…ƒåˆ°äººæ°‘å¸çš„å¤§è‡´æ±‡ç‡
        
        # åˆ›å»ºä½¿ç”¨è®°å½•
        TokenUsage.objects.create(
            user=user,
            model=self.model_config,
            conversation=conversation,
            message=message,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens,
            cost_usd=cost_usd,
            cost_rmb=cost_rmb,
            request_time=timezone.now(),
            response_time=response_time,
            is_successful=is_successful,
            error_message=error_message,
            metadata=metadata or {}
        )

class OpenAIService(ModelService):
    """OpenAIæ¨¡å‹æœåŠ¡"""
    def __init__(self, model_id: str):
        super().__init__(model_id)
        
    def generate_response(self, messages: List[Dict[str, str]], user=None, conversation=None, message=None) -> Tuple[str, Dict]:
        start_time = time.time()
        
        api_key = self.provider.api_key
        api_base = self.provider.api_base or "https://api.openai.com/v1"
        
        prompt_tokens = TokenCounter.count_message_tokens(messages, self.model_config.model_id)
        completion_tokens = 0
        usage_info = {"prompt_tokens": prompt_tokens, "completion_tokens": 0, "total_tokens": prompt_tokens}
        
        try:
            client = openai.OpenAI(
                api_key=api_key,
                base_url=api_base
            )
            
            response = client.chat.completions.create(
                model=self.model_config.model_id,
                messages=messages,
                max_tokens=self.model_config.max_tokens,
                temperature=self.model_config.temperature,
                top_p=self.model_config.top_p,
                presence_penalty=self.model_config.presence_penalty,
                frequency_penalty=self.model_config.frequency_penalty
            )
            
            content = response.choices[0].message.content or "æ— å“åº”å†…å®¹"
            
            # è·å–Tokenä½¿ç”¨æƒ…å†µ
            if hasattr(response, 'usage') and response.usage:
                usage_info = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
                completion_tokens = response.usage.completion_tokens
            else:
                # å¦‚æœOpenAIæ²¡æœ‰è¿”å›tokenä½¿ç”¨æƒ…å†µï¼Œåˆ™è®¡ç®—å›å¤æ–‡æœ¬çš„tokens
                completion_tokens = TokenCounter.count_tokens(content, self.model_config.model_id)
                usage_info["completion_tokens"] = completion_tokens
                usage_info["total_tokens"] = prompt_tokens + completion_tokens
            
            response_time = time.time() - start_time
            
            # è®°å½•ä½¿ç”¨æƒ…å†µ
            if user:
                self.record_token_usage(
                    user=user,
                    conversation=conversation,
                    message=message,
                    prompt_tokens=usage_info["prompt_tokens"],
                    completion_tokens=usage_info["completion_tokens"],
                    response_time=response_time,
                    metadata={"model_id": self.model_config.model_id}
                )
                
            return content, usage_info
            
        except Exception as e:
            response_time = time.time() - start_time
            error_message = str(e)
            
            # è®°å½•å¤±è´¥çš„è¯·æ±‚
            if user:
                self.record_token_usage(
                    user=user,
                    conversation=conversation,
                    message=message,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=0,
                    response_time=response_time,
                    is_successful=False,
                    error_message=error_message
                )
                
            raise

class AliyunService(ModelService):
    """é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹æœåŠ¡"""
    def __init__(self, model_id: str):
        super().__init__(model_id)
        print(f"   ğŸ”§ åˆå§‹åŒ–é˜¿é‡Œäº‘æœåŠ¡ï¼Œæ¨¡å‹ID: {model_id}")
        
    def generate_response(self, messages: List[Dict[str, str]], user=None, conversation=None, message=None) -> Tuple[str, Dict]:
        start_time = time.time()
        print(f"   ğŸš€ å¼€å§‹è°ƒç”¨é˜¿é‡Œäº‘æ¨¡å‹: {self.model_config.name}")
        
        api_key = self.provider.api_key
        # å…¼å®¹æ¨¡å¼çš„base_url
        base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
        
        prompt_tokens = TokenCounter.count_message_tokens(messages, "qwen")
        completion_tokens = 0
        usage_info = {"prompt_tokens": prompt_tokens, "completion_tokens": 0, "total_tokens": prompt_tokens}
        
        try:
            print(f"   ğŸ“¡ ä½¿ç”¨OpenAIå…¼å®¹æ¨¡å¼è¿æ¥é˜¿é‡Œäº‘API (æµå¼æ¨¡å¼)")
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯è¿æ¥é˜¿é‡Œäº‘
            from openai import OpenAI
            
            client = OpenAI(
                api_key=api_key,
                base_url=base_url
            )
            
            print(f"   ğŸ“¤ å‘é€æµå¼è¯·æ±‚åˆ°é˜¿é‡Œäº‘ï¼Œæ¶ˆæ¯æ•°: {len(messages)}")
            # ä½¿ç”¨æµå¼æ¨¡å¼åˆ›å»ºèŠå¤©å®Œæˆè¯·æ±‚
            completion = client.chat.completions.create(
                model=self.model_config.model_id,
                messages=messages,
                max_tokens=self.model_config.max_tokens,
                temperature=self.model_config.temperature,
                top_p=self.model_config.top_p,
                stream=True,  # å¯ç”¨æµå¼æ¨¡å¼
                stream_options={"include_usage": True}  # åŒ…å«ç”¨é‡ç»Ÿè®¡
            )
            
            # æ”¶é›†å®Œæ•´å“åº”
            full_content = ""
            reasoning_content = ""
            
            print(f"   ğŸ“¥ å¼€å§‹æ¥æ”¶æµå¼å“åº”...")
            for chunk in completion:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç”¨é‡ä¿¡æ¯
                if not hasattr(chunk, 'choices') or not chunk.choices:
                    if hasattr(chunk, 'usage') and chunk.usage:
                        usage_info = {
                            "prompt_tokens": chunk.usage.prompt_tokens,
                            "completion_tokens": chunk.usage.completion_tokens,
                            "total_tokens": chunk.usage.total_tokens
                        }
                        completion_tokens = chunk.usage.completion_tokens
                        print(f"   ğŸ“Š Tokenä½¿ç”¨æƒ…å†µ: {usage_info}")
                    continue
                
                delta = chunk.choices[0].delta
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æ€è€ƒè¿‡ç¨‹å†…å®¹
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:
                    reasoning_content += delta.reasoning_content
                    print(f"   ğŸ’­ æ¥æ”¶æ€è€ƒå†…å®¹: {len(reasoning_content)} å­—ç¬¦", end="\r")
                    
                # æ£€æŸ¥æ˜¯å¦æœ‰å›å¤å†…å®¹
                if hasattr(delta, 'content') and delta.content:
                    full_content += delta.content
                    print(f"   ğŸ’¬ æ¥æ”¶å›å¤å†…å®¹: {len(full_content)} å­—ç¬¦", end="\r")
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°ä½¿ç”¨æƒ…å†µï¼Œä¼°ç®—ä¸€ä¸‹
            if completion_tokens == 0:
                completion_tokens = TokenCounter.count_tokens(full_content, "qwen")
                usage_info["completion_tokens"] = completion_tokens
                usage_info["total_tokens"] = prompt_tokens + completion_tokens
                print(f"   ğŸ“Š ä¼°ç®—Tokenä½¿ç”¨æƒ…å†µ: {usage_info}")
            
            print(f"   âœ… æµå¼å“åº”æ¥æ”¶å®Œæˆï¼Œæ€»é•¿åº¦: {len(full_content)} å­—ç¬¦")
            
            response_time = time.time() - start_time
            print(f"   â±ï¸ å“åº”æ—¶é—´: {response_time:.2f}ç§’")
            
            # è®°å½•ä½¿ç”¨æƒ…å†µ
            if user:
                print(f"   ğŸ’¾ è®°å½•Tokenä½¿ç”¨æƒ…å†µ")
                self.record_token_usage(
                    user=user,
                    conversation=conversation,
                    message=message,
                    prompt_tokens=usage_info["prompt_tokens"],
                    completion_tokens=usage_info["completion_tokens"],
                    response_time=response_time,
                    metadata={
                        "model_id": self.model_config.model_id,
                        "has_reasoning": len(reasoning_content) > 0
                    }
                )
            
            # å¦‚æœæœ‰æ€è€ƒå†…å®¹ï¼Œå¯ä»¥é€‰æ‹©æ·»åŠ åˆ°å…ƒæ•°æ®æˆ–ç›´æ¥åˆå¹¶åˆ°å›å¤ä¸­
            if reasoning_content:
                print(f"   ğŸ“ æ¨¡å‹æä¾›äº†æ€è€ƒè¿‡ç¨‹ ({len(reasoning_content)} å­—ç¬¦)")
                # å¯ä»¥é€‰æ‹©ä¸‹é¢ä¸¤ç§æ–¹å¼ä¹‹ä¸€:
                
                # 1. å°†æ€è€ƒè¿‡ç¨‹æ·»åŠ åˆ°å›å¤å‰é¢
                # full_content = f"æ€è€ƒè¿‡ç¨‹ï¼š\n{reasoning_content}\n\nå›ç­”ï¼š\n{full_content}"
                
                # 2. åªä¿ç•™å›å¤å†…å®¹ï¼Œæ€è€ƒè¿‡ç¨‹ä½œä¸ºå…ƒæ•°æ®è®°å½•
                if message:
                    try:
                        # å‡è®¾Messageæ¨¡å‹æœ‰ä¸€ä¸ªmetadataå­—æ®µ
                        message.metadata = message.metadata or {}
                        message.metadata['reasoning'] = reasoning_content
                        message.save(update_fields=['metadata'])
                        print(f"   âœ… æ€è€ƒè¿‡ç¨‹å·²ä¿å­˜åˆ°æ¶ˆæ¯å…ƒæ•°æ®")
                    except:
                        print(f"   âš ï¸ æ— æ³•å°†æ€è€ƒè¿‡ç¨‹ä¿å­˜åˆ°å…ƒæ•°æ®")
                
            return full_content, usage_info
                
        except Exception as e:
            response_time = time.time() - start_time
            error_message = str(e)
            print(f"   âŒ é˜¿é‡Œäº‘APIé”™è¯¯: {error_message}")
            
            # è®°å½•å¤±è´¥çš„è¯·æ±‚
            if user:
                print(f"   ğŸ’¾ è®°å½•å¤±è´¥è¯·æ±‚")
                self.record_token_usage(
                    user=user,
                    conversation=conversation,
                    message=message,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=0,
                    response_time=response_time,
                    is_successful=False,
                    error_message=error_message
                )
                
            raise

class DeepSeekService(ModelService):
    """DeepSeekæ¨¡å‹æœåŠ¡"""
    def __init__(self, model_id: str):
        super().__init__(model_id)
        
    def generate_response(self, messages: List[Dict[str, str]], user=None, conversation=None, message=None) -> Tuple[str, Dict]:
        start_time = time.time()
        
        api_key = self.provider.api_key
        api_base = self.provider.api_base or "https://api.deepseek.com/v1"
        
        prompt_tokens = TokenCounter.count_message_tokens(messages, "deepseek")
        completion_tokens = 0
        usage_info = {"prompt_tokens": prompt_tokens, "completion_tokens": 0, "total_tokens": prompt_tokens}
        
        try:
            client = openai.OpenAI(
                api_key=api_key,
                base_url=api_base
            )
            
            response = client.chat.completions.create(
                model=self.model_config.model_id,
                messages=messages,
                max_tokens=self.model_config.max_tokens,
                temperature=self.model_config.temperature,
                top_p=self.model_config.top_p
            )
            
            content = response.choices[0].message.content or "æ— å“åº”å†…å®¹"
            
            # è·å–Tokenä½¿ç”¨æƒ…å†µ
            if hasattr(response, 'usage') and response.usage:
                usage_info = {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
                completion_tokens = response.usage.completion_tokens
            else:
                # å¦‚æœæ²¡æœ‰è¿”å›tokenä½¿ç”¨æƒ…å†µï¼Œåˆ™è®¡ç®—å›å¤æ–‡æœ¬çš„tokens
                completion_tokens = TokenCounter.count_tokens(content, "deepseek")
                usage_info["completion_tokens"] = completion_tokens
                usage_info["total_tokens"] = prompt_tokens + completion_tokens
            
            response_time = time.time() - start_time
            
            # è®°å½•ä½¿ç”¨æƒ…å†µ
            if user:
                self.record_token_usage(
                    user=user,
                    conversation=conversation,
                    message=message,
                    prompt_tokens=usage_info["prompt_tokens"],
                    completion_tokens=usage_info["completion_tokens"],
                    response_time=response_time,
                    metadata={"model_id": self.model_config.model_id}
                )
                
            return content, usage_info
            
        except Exception as e:
            response_time = time.time() - start_time
            error_message = str(e)
            
            # è®°å½•å¤±è´¥çš„è¯·æ±‚
            if user:
                self.record_token_usage(
                    user=user,
                    conversation=conversation,
                    message=message,
                    prompt_tokens=prompt_tokens,
                    completion_tokens=0,
                    response_time=response_time,
                    is_successful=False,
                    error_message=error_message
                )
                
            raise

def get_ai_response(conversation, user_message: str, model_id: str = None, user=None, existing_message_id=None) -> str:
    """ç»Ÿä¸€æ¥å£ï¼Œä»æŒ‡å®šå¤§æ¨¡å‹è·å–å›å¤"""
    # è·å–ç”¨æˆ·åå¥½çš„æ¨¡å‹ï¼Œå¦‚æœªæŒ‡å®šåˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
    if not model_id:
        if user and hasattr(user, 'preferred_model'):
            model_id = user.preferred_model
        else:
            model_id = ModelService.get_default_model()
    
    # æ„å»ºå¯¹è¯å†å²
    messages = []
    
    # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
    messages.append({
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åŠ©æ‰‹ï¼Œå¸®åŠ©ç”¨æˆ·å›ç­”é—®é¢˜å¹¶æä¾›å‡†ç¡®çš„ä¿¡æ¯ã€‚"
    })
    
    # æ·»åŠ å†å²æ¶ˆæ¯
    history = conversation.messages.order_by('-timestamp')[:10]
    for msg in reversed(list(history)):
        messages.append({
            "role": msg.role,
            "content": msg.content
        })
    
    # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
    messages.append({
        "role": "user",
        "content": user_message
    })
    
    # åå¤‡å“åº”
    fallback_responses = [
        "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚è¯·ç¨åå†è¯•ã€‚",
        "ç”±äºæŠ€æœ¯åŸå› ï¼Œæ— æ³•å¤„ç†æ‚¨çš„è¯·æ±‚ã€‚æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ä¿®å¤é—®é¢˜ã€‚",
        "è¿æ¥AIæœåŠ¡æ—¶é‡åˆ°é—®é¢˜ã€‚è¯·ç¨åé‡è¯•æˆ–å°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹ã€‚"
    ]
    
    # è·å–å·²å­˜åœ¨çš„ç”¨æˆ·æ¶ˆæ¯å¯¹è±¡ï¼Œè€Œä¸æ˜¯åˆ›å»ºæ–°çš„
    from knowledge.models import Message
    
    if existing_message_id:
        # å¦‚æœæä¾›äº†ç°æœ‰æ¶ˆæ¯IDï¼Œç›´æ¥è·å–
        try:
            user_message_obj = Message.objects.get(id=existing_message_id)
            print(f"[DEBUG] ä½¿ç”¨å·²å­˜åœ¨çš„ç”¨æˆ·æ¶ˆæ¯ ID: {existing_message_id}")
        except Message.DoesNotExist:
            print(f"[WARNING] æ‰¾ä¸åˆ°æŒ‡å®šIDçš„æ¶ˆæ¯: {existing_message_id}ï¼Œå°†åˆ›å»ºæ–°æ¶ˆæ¯")
            user_message_obj = None
    else:
        # å°è¯•æŸ¥æ‰¾æœ€è¿‘çš„åŒ¹é…æ¶ˆæ¯
        import hashlib
        message_hash = hashlib.md5(user_message.encode()).hexdigest()
        user_message_obj = Message.objects.filter(
            conversation=conversation,
            role='user',
            message_hash=message_hash
        ).order_by('-timestamp').first()
        
        print(f"[DEBUG] æŸ¥æ‰¾åŒ¹é…æ¶ˆæ¯ hash: {message_hash[:8]}, ç»“æœ: {'æ‰¾åˆ°' if user_message_obj else 'æœªæ‰¾åˆ°'}")
    
    # åªæœ‰åœ¨ç¡®å®æ‰¾ä¸åˆ°ç°æœ‰æ¶ˆæ¯æ—¶æ‰åˆ›å»ºæ–°æ¶ˆæ¯
    if not user_message_obj:
        print("[WARNING] æœªæ‰¾åˆ°åŒ¹é…çš„ç”¨æˆ·æ¶ˆæ¯ï¼Œåˆ›å»ºæ–°æ¶ˆæ¯")
        user_message_obj = Message.objects.create(
            conversation=conversation,
            role='user',
            content=user_message,
            message_hash=message_hash if 'message_hash' in locals() else None
        )
    
    try:
        # åªä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼Œä¸å†å°è¯•å¤‡é€‰æ¨¡å‹
        service = ModelService.get_service(model_id)
        
        # ç”Ÿæˆå“åº”
        content, usage_info = service.generate_response(
            messages, 
            user=user or conversation.user, 
            conversation=conversation,
            message=user_message_obj
        )
        
        # åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯ - ä¸ä½¿ç”¨metadataå‚æ•°
        assistant_message = Message.objects.create(
            conversation=conversation,
            role='assistant',
            content=content
        )
        
        return content
            
    except Exception as e:
        print(f"æ¨¡å‹ {model_id} è°ƒç”¨å¤±è´¥: {e}")
        # è¿”å›åå¤‡å“åº”
        import random
        fallback_response = random.choice(fallback_responses)
        
        # åˆ›å»ºå¤±è´¥çš„åŠ©æ‰‹æ¶ˆæ¯ - ä¸ä½¿ç”¨metadataå‚æ•°
        Message.objects.create(
            conversation=conversation,
            role='assistant',
            content=fallback_response
        )
        
        return fallback_response
